{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk2XiHDfedP3XP8PQwIXY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andyfer004/HT2-IA/blob/main/HT2_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNIVERSIDAD DEL VALLE DE GUATEMALA  \n",
        "## CC 3085 – Inteligencia Artificial  \n",
        "### Sección 10  \n",
        "### Luis Alberto Suriano  \n",
        "\n",
        "\n",
        "# Hoja de trabajo 12\n",
        "\n",
        "**Integrantes:**  \n",
        "- Andy Fuentes 22944\n",
        "- Diederich Solís 22952\n",
        "- Davis Roldan 22672\n",
        "\n",
        "Guatemala, enero del 2025  \n"
      ],
      "metadata": {
        "id": "LEdsmBx1AI_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1 - Preguntas Teóricas**\n",
        "\n",
        "## 1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
        "\n",
        "Un **Proceso de Decisión de Markov (MDP)** es un modelo matemático utilizado para la toma de decisiones en entornos inciertos. Sus componentes son:\n",
        "\n",
        "- **Estados (S):** Conjunto de posibles estados en los que puede estar el agente.\n",
        "- **Acciones (A):** Conjunto de acciones posibles que el agente puede tomar en cada estado.\n",
        "- **Función de transición (T(s, a, s')):** Probabilidad de que el sistema pase del estado `s` al estado `s'` al tomar la acción `a`.\n",
        "- **Recompensa (R(s, a, s')):** Beneficio obtenido al realizar la transición de `s` a `s'` mediante la acción `a`.\n",
        "- **Factor de descuento (γ):** Parámetro entre 0 y 1 que pondera la importancia de las recompensas futuras.\n",
        "- **Estado final (IsEnd(s)):** Determina si un estado es terminal.\n",
        "\n",
        "## 2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.\n",
        "\n",
        "- **Política (π):** Es una estrategia que mapea cada estado `s` con una acción `a`, determinando qué acción tomar en cada estado.\n",
        "- **Evaluación de políticas:** Proceso de cálculo de la utilidad esperada de una política dada, utilizando la ecuación de Bellman.\n",
        "- **Mejora de políticas:** Proceso de optimización en el cual se ajusta la política para obtener mejores valores de recompensa esperada.\n",
        "- **Iteración de políticas:** Algoritmo que combina evaluación y mejora de políticas hasta que se encuentra una política óptima.\n",
        "\n",
        "## 3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
        "\n",
        "El **factor de descuento (γ)** determina cuánto valor tienen las recompensas futuras en comparación con las inmediatas. Su impacto es el siguiente:\n",
        "\n",
        "- **γ = 1:** Se da igual peso a todas las recompensas futuras (enfoque a largo plazo).\n",
        "- **γ = 0:** Solo importa la recompensa inmediata (enfoque miope).\n",
        "- **0 < γ < 1:** Se equilibran recompensas a corto y largo plazo.\n",
        "\n",
        "Un γ alto fomenta decisiones orientadas al futuro, mientras que un γ bajo prioriza ganancias inmediatas.\n",
        "\n",
        "## 4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
        "\n",
        "- **Iteración de valores:** Se enfoca en calcular el valor óptimo de cada estado y deducir la mejor política a partir de estos valores.\n",
        "- **Iteración de políticas:** Alterna entre evaluar una política y mejorarla hasta que se alcanza una política óptima.\n",
        "\n",
        "La **iteración de valores** es más directa para encontrar el valor óptimo, mientras que la **iteración de políticas** es más eficiente en casos donde la política óptima converge rápidamente.\n",
        "\n",
        "## 5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.\n",
        "\n",
        "### Desafíos:\n",
        "- **Explosión del espacio de estados:** A medida que el número de estados y acciones crece, el cálculo se vuelve ineficiente.\n",
        "- **Costos computacionales elevados:** Evaluar y mejorar políticas en un gran espacio de estados es costoso en términos de memoria y tiempo.\n",
        "- **Dificultad en la estimación de probabilidades de transición:** En entornos complejos, modelar correctamente las probabilidades de transición es un reto.\n",
        "\n",
        "### Enfoques para abordar estos desafíos:\n",
        "- **Métodos de aproximación:** Uso de **aprendizaje por refuerzo** y técnicas como **DQN** o **Monte Carlo** para estimar valores sin almacenar todos los estados.\n",
        "- **Representación compacta del estado:** Uso de funciones de valor parametrizadas o redes neuronales.\n",
        "- **Métodos de planificación jerárquica:** Descomponer problemas en subproblemas más manejables.\n",
        "\n"
      ],
      "metadata": {
        "id": "PnYhed1__NPU"
      }
    }
  ]
}